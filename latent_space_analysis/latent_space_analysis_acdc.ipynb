{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import spearmanr\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "derivatives_path = '/media/jaume/DATA/Data/New_ACDC/MIDS/mixed/derivatives'\n",
    "experiment_name = 'GraphClassification'\n",
    "\n",
    "# Get the list of subjects\n",
    "biomarkers_filename = os.path.join(derivatives_path, 'biomarkers.csv')\n",
    "df_biomarkers = pd.read_csv(biomarkers_filename, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== ACDC\n",
    "derivatives_folder = '/media/jaume/DATA/Data/New_ACDC/MIDS/mixed/derivatives'\n",
    "data_path = os.path.join(derivatives_folder, 'GraphClassification')\n",
    "all_edges = 'Edges-True_Norm-ZNorm_Global-True_All-True_Sim-False_BP-False'\n",
    "aha_edges = 'Edges-True_Norm-ZNorm_Global-True_All-False_Sim-False_BP-False'\n",
    "# study_name = 'Multiplex_HPT_ACDC_ADAM_FINAL_MAE'\n",
    "\n",
    "# =========================== ALL ===========================\n",
    "study_name = 'Multiplex_HPT_ACDC_DIMENSIONS_ALL_SUM'\n",
    "all_data_folder = os.path.join(data_path, all_edges, study_name)  # save_folder\n",
    "\n",
    "# =========================== AHA ===========================\n",
    "study_name = 'Multiplex_HPT_ACDC_DIMENSIONS_SUM'\n",
    "aha_data_folder = os.path.join(data_path, aha_edges, study_name)  # save_folder\n",
    "\n",
    "save_folder = os.path.join(all_data_folder, 'latent_analysis') \n",
    "latent_filename = os.path.join(all_data_folder, 'latent_data.csv')\n",
    "save_folder = os.path.join(aha_data_folder, 'latent_analysis') \n",
    "latent_filename = os.path.join(aha_data_folder, 'latent_data.csv')\n",
    "os.makedirs(save_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_latent = pd.read_csv(latent_filename)\n",
    "df_latent['Subject'] = df_biomarkers['Subject'].copy()\n",
    "\n",
    "drop_columns=['Region', 'Cycle', 'dt', 'ed_cycle_time', 'es_cycle_time', 'ed_frame_idx', 'es_frame_idx', 'Group', 'Height', 'Weight', 'BMI']\n",
    "drop_columns_biomarkers = ['LV_SV', 'LV_Myo_EF', 'LV_Myo_SV', 'RV_SV', 'RV_Myo_SV', 'RV_Myo_EF', 'RV_Myo_SVI', 'LV_Myo_SVI']\n",
    "df_biomarkers[['RVMI']] = df_biomarkers[['RV_Myo_SVI']] * 1.05\n",
    "df_biomarkers[['LVMI']] = df_biomarkers[['LV_Myo_SVI']] * 1.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biomarkers [targets]\n",
    "y_df = df_biomarkers.copy()\n",
    "y_df.dropna(axis=1, inplace=True)\n",
    "y_df.drop(columns=drop_columns, inplace=True)  # Remove non-feature columns\n",
    "y_df.drop(columns=drop_columns_biomarkers, inplace=True)\n",
    "y_df.set_index('Subject', inplace=True)\n",
    "feature_names = y_df.columns.tolist()\n",
    "\n",
    "# Latent data\n",
    "X_df = df_latent.copy()\n",
    "X_df.drop(columns=['Sample', 'labels'], inplace=True)  # Remove non-feature columns\n",
    "X_df.set_index('Subject', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: PCA on X_df === Too much otherwise\n",
    "X_df_clean = X_df.dropna(axis=1, thresh=int(0.9 * len(X_df)))  # optional: remove columns with too many NaNs\n",
    "X_df_filled = X_df_clean.fillna(X_df_clean.mean())  # mean impute if necessary\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_df_filled)\n",
    "\n",
    "pca = PCA()\n",
    "X_pca_full = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Step 2: Choose number of components explaining target variance (e.g., 95%)\n",
    "explained_var_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "n_components = np.argmax(explained_var_ratio >= 0.95) + 1\n",
    "\n",
    "print(f\"Selected {n_components} components to explain 95% of the variance.\")\n",
    "\n",
    "X_pca = pd.DataFrame(X_pca_full[:, :n_components],\n",
    "                     index=X_df.index,\n",
    "                     columns=[f\"PC{i+1}\" for i in range(n_components)])\n",
    "\n",
    "# Step 3: Join with y_df\n",
    "df_joined = X_pca.copy().join(y_df, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Compute Spearman correlations: each PC vs each Y variable\n",
    "results = []\n",
    "for pc in X_pca.columns:\n",
    "    for yvar in y_df.columns:\n",
    "        if pc not in df_joined.columns or yvar not in df_joined.columns:\n",
    "            continue\n",
    "        subset = df_joined[[pc, yvar]].dropna()\n",
    "        if len(subset) < 10:\n",
    "            continue\n",
    "        x = subset[pc]\n",
    "        y = subset[yvar]\n",
    "        if x.nunique() < 2 or y.nunique() < 2:\n",
    "            continue\n",
    "        r, p = spearmanr(x, y)\n",
    "        results.append({'PC': pc, 'Y_var': yvar, 'Correlation': r, 'p_value': p})\n",
    "\n",
    "assoc_df = pd.DataFrame(results)\n",
    "\n",
    "# Step 4: FDR correction (Benjamini-Hochberg)\n",
    "pvals = assoc_df['p_value'].values\n",
    "rejected, pvals_corrected, _, _ = multipletests(pvals, method='fdr_bh', alpha=0.05)\n",
    "assoc_df['p_value_corrected'] = pvals_corrected\n",
    "assoc_df['Significant'] = rejected\n",
    "assoc_df['Significant_NC'] = pvals < 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assoc_df.sort_values(by='p_value', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ Permutation testing ===============\n",
    "\n",
    "# Params\n",
    "n_jobs = 3\n",
    "n_permutations = 200\n",
    "alphas=[0.01, 0.05, 0.1, 0.5, 1, 5, 10]\n",
    "\n",
    "def permutation_test(target):\n",
    "    y = y_df[target].values.squeeze()\n",
    "    r2_permute = []\n",
    "    X_used = X_pca.copy()\n",
    "\n",
    "    # Fit model on original data\n",
    "    search = GridSearchCV(make_pipeline(StandardScaler(), Ridge()), \n",
    "                          param_grid={'ridge__alpha': alphas}, scoring='r2', cv=5, n_jobs=3)\n",
    "    search.fit(X_used, y)\n",
    "    final_params = {key.split('__')[1]: value for key, value in search.best_params_.items()}\n",
    "\n",
    "    # Fit model with best params\n",
    "    model = make_pipeline(StandardScaler(), Ridge(**final_params))\n",
    "    model.fit(X_used, y)        \n",
    "    y_pred = model.predict(X_used)\n",
    "    r2_train = r2_score(y, y_pred)\n",
    "\n",
    "    # Permutation loop\n",
    "    for _ in range(n_permutations):\n",
    "        y_perm = np.random.permutation(y.squeeze())\n",
    "        model_perm = make_pipeline(StandardScaler(), Ridge(**final_params))\n",
    "        model_perm.fit(X_used, y_perm)\n",
    "        r2_permute.append(r2_score(y_perm, model_perm.predict(X_used)))\n",
    "\n",
    "    # Compute p-value\n",
    "    p_value = (1 + np.sum(np.array(r2_permute) >= r2_train)) / (1 + len(r2_permute))\n",
    "\n",
    "\n",
    "    return {\n",
    "        'target': target,\n",
    "        'r2_train': r2_train,\n",
    "        'r2_perm_mean': np.mean(r2_permute),\n",
    "        'r2_perm_std': np.std(r2_permute),\n",
    "        'r2_permute_all': r2_permute,\n",
    "        'p_value': p_value,\n",
    "    }\n",
    "\n",
    "# Run in parallel\n",
    "results = Parallel(n_jobs=n_jobs)(\n",
    "    delayed(permutation_test)(target) for target in tqdm(y_df.columns, desc=\"Permutation test\")\n",
    ")\n",
    "\n",
    "# Summary table\n",
    "summary_df = pd.DataFrame({\n",
    "    res['target']: {\n",
    "        'R2_train': res['r2_train'],\n",
    "        'R2_perm_mean': res['r2_perm_mean'],\n",
    "        'R2_perm_std': res['r2_perm_std'],\n",
    "        'p_value': res['p_value']\n",
    "    }\n",
    "    for res in results\n",
    "}).T\n",
    "\n",
    "# Full permutation R2s (for plotting)\n",
    "r2_perm_full_df = pd.DataFrame({\n",
    "    res['target']: pd.Series(res['r2_permute_all'])  # pad shorter series with NaN if needed\n",
    "    for res in results\n",
    "})\n",
    "\n",
    "# Save\n",
    "summary_df.to_csv(f\"{save_folder}/rf_permutation_summary_pca.csv\")\n",
    "r2_perm_full_df.to_csv(f\"{save_folder}/rf_permutation_r2_distributions_pca.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply multiple testing correction (FDR)\n",
    "summary_df[\"fdr_corrected_p\"] = multipletests(summary_df[\"p_value\"], method=\"fdr_bh\")[1]\n",
    "summary_df[\"sig_marker\"] = summary_df[\"p_value\"].apply(lambda p: \"*\" if p < 0.1 else \"\")\n",
    "summary_df.loc[summary_df[\"fdr_corrected_p\"] < 0.1, \"sig_marker\"] = \"**\"\n",
    "summary_df\n",
    "\n",
    "# Compute values for volcano plot\n",
    "summary_df[\"r2_diff\"] = summary_df[\"R2_train\"] - summary_df[\"R2_perm_mean\"]\n",
    "summary_df[\"neg_log_p\"] = -np.log10(summary_df[\"p_value\"])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = summary_df[\"p_value\"] < 0.1\n",
    "plt.scatter(summary_df[\"r2_diff\"], summary_df[\"neg_log_p\"], c=colors.map({True: \"red\", False: \"gray\"}))\n",
    "plt.axhline(-np.log10(0.1), linestyle=\"--\", color=\"black\", linewidth=1)\n",
    "plt.xlabel(\"ΔR² (Train - Permuted Mean)\")\n",
    "plt.ylabel(\"-log₁₀(p-value)\")\n",
    "plt.title(\"Volcano Plot: Predictive Gain vs. Statistical Significance\")\n",
    "\n",
    "# Annotate top features (optional)\n",
    "top = summary_df[summary_df[\"p_value\"] < 0.1].sort_values(\"neg_log_p\", ascending=False).head(5)\n",
    "for idx, row in top.iterrows():\n",
    "    plt.text(row[\"r2_diff\"], row[\"neg_log_p\"] + 0.1, idx, fontsize=8, ha='center')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "summary_df.sort_values(by='p_value', ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Create heatmap data (only significant correlations)\n",
    "heatmap_data = assoc_df[assoc_df['Significant']].pivot(index='Y_var', columns='PC', values='Correlation')\n",
    "heatmap_data = assoc_df[assoc_df['Significant_NC']].pivot(index='Y_var', columns='PC', values='Correlation')\n",
    "\n",
    "# Drop rows/columns with all NaNs\n",
    "heatmap_data = heatmap_data.dropna(how='all', axis=0).dropna(how='all', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assoc_df.to_csv(os.path.join(save_folder, 'association_acdc_pc.csv'))\n",
    "assoc_df.sort_values(by='p_value', ascending=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyMultiplex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
