{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import spearmanr\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data_folder = '/home/jaume/Desktop/Data/New_ACDC/MIDS/mixed/derivatives/'\n",
    "derivatives_path = \"/media/jaume/DATA/Data/Urblauna_SFTP/UKB_Cardiac_BIDS/derivatives\"\n",
    "experiment_name = 'GraphClassification'\n",
    "\n",
    "# Get the list of subjects\n",
    "biomarkers_filename = os.path.join(derivatives_path, 'biomarkers.csv')\n",
    "df_biomarkers = pd.read_csv(biomarkers_filename, index_col=0)\n",
    "\n",
    "metadata_filename = os.path.join(derivatives_path, 'metadata_participants_numeric.csv')\n",
    "df_numeric_metadata = pd.read_csv(metadata_filename, index_col=0)\n",
    "\n",
    "merged_filename = os.path.join(derivatives_path, 'metadata_participants_biomarkers.csv')\n",
    "df_merged = pd.read_csv(merged_filename, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== UKB\n",
    "derivatives_folder = \"/media/jaume/DATA/Data/Urblauna_SFTP/UKB_Cardiac_BIDS/derivatives\"\n",
    "data_path = os.path.join(derivatives_folder, 'GraphClassification')\n",
    "all_edges = 'Edges-True_Norm-ZNorm_Global-True_All-True_Sim-False_BP-False'\n",
    "aha_edges = 'Edges-True_Norm-ZNorm_Global-True_All-False_Sim-False_BP-False'\n",
    "# study_name = 'Multiplex_HPT_ACDC_ADAM_FINAL_MAE'\n",
    "\n",
    "# =========================== ALL ===========================\n",
    "study_name = 'Multiplex_HPT_UKB_DIMENSIONS_NEW_LOSS_ALL'\n",
    "all_data_folder = os.path.join(data_path, all_edges, study_name)  # save_folder\n",
    "\n",
    "# =========================== AHA ===========================\n",
    "study_name = 'Multiplex_HPT_UKB_DIMENSIONS_NEW_LOSS'\n",
    "aha_data_folder = os.path.join(data_path, aha_edges, study_name)  # save_folder\n",
    "\n",
    "save_folder = os.path.join(all_data_folder, 'latent_analysis') \n",
    "latent_filename = os.path.join(all_data_folder, 'latent_data.csv')\n",
    "save_folder = os.path.join(aha_data_folder, 'latent_analysis') \n",
    "latent_filename = os.path.join(aha_data_folder, 'latent_data.csv')\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_latent = pd.read_csv(latent_filename)\n",
    "df_latent['Subject'] = df_biomarkers['Subject'].copy()\n",
    "drop_columns=['Region', 'Cycle', 'dt', 'ed_cycle_time', 'es_cycle_time', 'ed_frame_idx', 'es_frame_idx', 'Height']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.drop(columns=['Group'], inplace=True)\n",
    "drop_columns_biomarkers = ['LV_SV', 'LV_Myo_EF', 'LV_Myo_SV', 'RV_SV', 'RV_Myo_SV', 'RV_Myo_EF', 'RV_Myo_SVI', 'LV_Myo_SVI']\n",
    "df_merged[['RVMI']] = df_merged[['RV_Myo_SVI']] * 1.05\n",
    "df_merged[['LVMI']] = df_merged[['LV_Myo_SVI']] * 1.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for the regression analysis\n",
    "y_df = df_merged.copy()\n",
    "y_df.drop(columns=drop_columns, inplace=True)  # Remove non-feature columns\n",
    "y_df.drop(columns=drop_columns_biomarkers, inplace=True)\n",
    "y_df.set_index('Subject', inplace=True)\n",
    "\n",
    "missing_counts = y_df.isna().sum()\n",
    "print(list(missing_counts[missing_counts > 0].sort_values().index))\n",
    "\n",
    "y_df = y_df.loc[:, y_df.isnull().mean() < 0.3]\n",
    "# imputer = KNNImputer(n_neighbors=5)\n",
    "# y_df_imputed = pd.DataFrame(imputer.fit_transform(y_df), columns=y_df.columns, index=y_df.index)\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "y_df_imputed = pd.DataFrame(imputer.fit_transform(y_df), columns=y_df.columns, index=y_df.index)\n",
    "feature_names = y_df.columns.tolist()\n",
    "y_df = y_df_imputed.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for the regression analysis\n",
    "X_df = df_latent.copy()\n",
    "X_df.drop(columns=['Sample', 'labels'], inplace=True)  # Remove non-feature columns\n",
    "X_df.set_index('Subject', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: PCA on X_df === Too much otherwise\n",
    "X_df_clean = X_df.dropna(axis=1, thresh=int(0.9 * len(X_df)))  # optional: remove columns with too many NaNs\n",
    "X_df_filled = X_df_clean.fillna(X_df_clean.mean())  # mean impute if necessary\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_df_filled)\n",
    "\n",
    "pca = PCA()\n",
    "X_pca_full = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Step 2: Choose number of components explaining target variance (e.g., 95%)\n",
    "explained_var_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "n_components = np.argmax(explained_var_ratio >= 0.95) + 1\n",
    "\n",
    "print(f\"Selected {n_components} components to explain 95% of the variance.\")\n",
    "\n",
    "X_pca = pd.DataFrame(X_pca_full[:, :n_components],\n",
    "                     index=X_df.index,\n",
    "                     columns=[f\"PC{i+1}\" for i in range(n_components)])\n",
    "\n",
    "# Step 3: Join with y_df\n",
    "df_joined = X_pca.join(y_df, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Compute Spearman correlations: each PC vs each Y variable\n",
    "results = []\n",
    "for pc in X_pca.columns:\n",
    "    for yvar in y_df.columns:\n",
    "        if pc not in df_joined.columns or yvar not in df_joined.columns:\n",
    "            continue\n",
    "        subset = df_joined[[pc, yvar]].dropna()\n",
    "        if len(subset) < 10:\n",
    "            continue\n",
    "        x = subset[pc]\n",
    "        y = subset[yvar]\n",
    "        if x.nunique() < 2 or y.nunique() < 2:\n",
    "            continue\n",
    "        r, p = spearmanr(x, y)\n",
    "        results.append({'PC': pc, 'Y_var': yvar, 'Correlation': r, 'p_value': p})\n",
    "\n",
    "assoc_df = pd.DataFrame(results)\n",
    "\n",
    "# Step 4: FDR correction (Benjamini-Hochberg)\n",
    "pvals = assoc_df['p_value'].values\n",
    "rejected, pvals_corrected, _, _ = multipletests(pvals, method='fdr_bh', alpha=0.05)\n",
    "assoc_df['p_value_corrected'] = pvals_corrected\n",
    "assoc_df['Significant'] = rejected\n",
    "assoc_df['Significant_NC'] = pvals < 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assoc_df.sort_values(by='p_value', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "n_jobs = 3\n",
    "n_permutations = 200\n",
    "alphas=[0.01, 0.05, 0.1, 0.5, 1, 5, 10]\n",
    "\n",
    "def permutation_test(target):\n",
    "    y = y_df[target].values.squeeze()\n",
    "    r2_permute = []\n",
    "    X_used = X_pca.copy()\n",
    "\n",
    "    # Rigid    \n",
    "    search = GridSearchCV(make_pipeline(StandardScaler(), Ridge()), \n",
    "                          param_grid={'ridge__alpha': alphas}, scoring='r2', cv=5, n_jobs=3)\n",
    "    search.fit(X_used, y)\n",
    "    final_params = {key.split('__')[1]: value for key, value in search.best_params_.items()}\n",
    "\n",
    "    # Fit model\n",
    "    model = make_pipeline(StandardScaler(), Ridge(**final_params))\n",
    "    model.fit(X_used, y)\n",
    "    y_pred = model.predict(X_used)\n",
    "    r2_train = r2_score(y, y_pred)\n",
    "\n",
    "    # Permutation loop\n",
    "    for _ in range(n_permutations):\n",
    "        y_perm = np.random.permutation(y.squeeze())\n",
    "        model_perm = make_pipeline(StandardScaler(), Ridge(**final_params))\n",
    "        model_perm.fit(X_used, y_perm)\n",
    "        r2_permute.append(r2_score(y_perm, model_perm.predict(X_used)))\n",
    "\n",
    "    # Compute p-value\n",
    "    p_value = (1 + np.sum(np.array(r2_permute) >= r2_train)) / (1 + len(r2_permute))\n",
    "\n",
    "\n",
    "    return {\n",
    "        'target': target,\n",
    "        'r2_train': r2_train,\n",
    "        'r2_perm_mean': np.mean(r2_permute),\n",
    "        'r2_perm_std': np.std(r2_permute),\n",
    "        'r2_permute_all': r2_permute,  # full list\n",
    "        'p_value': p_value,\n",
    "    }\n",
    "\n",
    "# Run in parallel\n",
    "results = Parallel(n_jobs=n_jobs)(\n",
    "    delayed(permutation_test)(target) for target in tqdm(y_df.columns, desc=\"Permutation test\")\n",
    ")\n",
    "\n",
    "# Summary table\n",
    "summary_df = pd.DataFrame({\n",
    "    res['target']: {\n",
    "        'R2_train': res['r2_train'],\n",
    "        'R2_perm_mean': res['r2_perm_mean'],\n",
    "        'R2_perm_std': res['r2_perm_std'],\n",
    "        'p_value': res['p_value']\n",
    "    }\n",
    "    for res in results\n",
    "}).T\n",
    "\n",
    "# Full permutation R2s (for plotting)\n",
    "r2_perm_full_df = pd.DataFrame({\n",
    "    res['target']: pd.Series(res['r2_permute_all'])  # pad shorter series with NaN if needed\n",
    "    for res in results\n",
    "})\n",
    "\n",
    "# Save\n",
    "summary_df.to_csv(f\"{save_folder}/rf_permutation_summary_pca.csv\")\n",
    "r2_perm_full_df.to_csv(f\"{save_folder}/rf_permutation_r2_distributions_pca.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Apply multiple testing correction (FDR)\n",
    "summary_df[\"fdr_corrected_p\"] = multipletests(summary_df[\"p_value\"], method=\"fdr_bh\")[1]\n",
    "summary_df[\"sig_marker\"] = summary_df[\"p_value\"].apply(lambda p: \"*\" if p < 0.1 else \"\")\n",
    "summary_df.loc[summary_df[\"fdr_corrected_p\"] < 0.1, \"sig_marker\"] = \"**\"\n",
    "summary_df\n",
    "\n",
    "# Compute values for volcano plot\n",
    "summary_df[\"r2_diff\"] = summary_df[\"R2_train\"] - summary_df[\"R2_perm_mean\"]\n",
    "summary_df[\"neg_log_p\"] = -np.log10(summary_df[\"p_value\"])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = summary_df[\"p_value\"] < 0.1\n",
    "plt.scatter(summary_df[\"r2_diff\"], summary_df[\"neg_log_p\"], c=colors.map({True: \"red\", False: \"gray\"}))\n",
    "plt.axhline(-np.log10(0.1), linestyle=\"--\", color=\"black\", linewidth=1)\n",
    "plt.xlabel(\"ΔR² (Train - Permuted Mean)\")\n",
    "plt.ylabel(\"-log₁₀(p-value)\")\n",
    "plt.title(\"Volcano Plot: Predictive Gain vs. Statistical Significance\")\n",
    "\n",
    "# Annotate top features (optional)\n",
    "top = summary_df[summary_df[\"p_value\"] < 0.1].sort_values(\"neg_log_p\", ascending=False).head(5)\n",
    "for idx, row in top.iterrows():\n",
    "    plt.text(row[\"r2_diff\"], row[\"neg_log_p\"] + 0.1, idx, fontsize=8, ha='center')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "summary_df.sort_values(by='p_value', ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assoc_df.to_csv(os.path.join(save_folder, 'association_ukb_pc.csv'))\n",
    "assoc_df.sort_values(by='p_value', ascending=True)\n",
    "assoc_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyMultiplex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
